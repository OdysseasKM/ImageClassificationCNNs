{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING AND SHOWING IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "from array import array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MnistDataloader:\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "\n",
    "    def read_images_labels(self, images_filepath, labels_filepath):\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2049, got {magic}')\n",
    "            labels = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2051, got {magic}')\n",
    "            images = np.frombuffer(file.read(), dtype=np.uint8).reshape(size, rows, cols)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "training_images_filepath = os.path.join(working_dir, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = os.path.join(working_dir, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = os.path.join(working_dir, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = os.path.join(working_dir, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "\n",
    "print('Loading MNIST dataset...')\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "print('MNIST dataset loaded.')\n",
    "\n",
    "\n",
    "def show_images(images, title_texts, cols=5):\n",
    "    rows = len(images) // cols + (len(images) % cols != 0)\n",
    "    plt.figure(figsize=(15, 3 * rows))\n",
    "    for i, (image, title) in enumerate(zip(images, title_texts)):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_one_image_per_class(images, labels):\n",
    "    class_dict = {i: None for i in range(10)}  # Dictionary to store one image per class\n",
    "    filled_classes = set()\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        if class_dict[label] is None:\n",
    "            class_dict[label] = image\n",
    "            filled_classes.add(label)\n",
    "        if len(filled_classes) == 10:\n",
    "            break\n",
    "\n",
    "    return class_dict\n",
    "\n",
    "# Get one image per class from the training set\n",
    "class_images_train = get_one_image_per_class(x_train, y_train)\n",
    "train_images, train_titles = zip(*[(img, f'training image of {i}') for i, img in class_images_train.items()])\n",
    "\n",
    "\n",
    "# Show images from training set\n",
    "show_images(train_images, train_titles, cols=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING AND VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into 80% training and 20% validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the sizes of the resulting datasets\n",
    "print(f'Training set:   {len(x_train)} images')\n",
    "print(f'Validation set: {len(x_valid)} images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Downsample the original training data to 50%\n",
    "x_train_half, _, y_train_half, _ = train_test_split(x_train, y_train, test_size=0.95, random_state=42)\n",
    "\n",
    "# Now split the downsampled data into 80% training and 20% validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_half, y_train_half, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the sizes of the resulting datasets\n",
    "print(f'Training set:   {len(x_train)} images')\n",
    "print(f'Validation set: {len(x_valid)} images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define batch size and other hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 35\n",
    "\n",
    "# Convert data to PyTorch Tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_valid_tensor = torch.tensor(x_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create TensorDataset and DataLoader for validation\n",
    "valid_dataset = TensorDataset(x_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 12 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.unsqueeze(1))  # Add channel dimension for grayscale images\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data.unsqueeze(1))  # Add channel dimension for grayscale images\n",
    "            loss = criterion(output, target)\n",
    "            total_valid_loss += loss.item()\n",
    "    \n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}')\n",
    "    \n",
    "    # Save the model with the best validation loss\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# Load the best model state dict\n",
    "if best_model_state_dict is not None:\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    print(best_valid_loss)\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools  # Import itertools module for iterating over combinations\n",
    "\n",
    "\n",
    "\n",
    "# Define DataLoader for the test set (using your loaded data)\n",
    "test_dataset = list(zip(x_test, y_test))  # Create a list of tuples (image, label)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize your model (SimpleCNN) and load the best trained weights\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test set and collect predictions and labels\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float()  # Convert input images to float type\n",
    "        outputs = model(images.unsqueeze(1))  # Unsqueeze to add channel dimension (assuming your model expects a single channel)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, range(10))\n",
    "plt.yticks(tick_marks, range(10))\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HoG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "\n",
    "\n",
    "def compute_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        # Compute HoG features for each image\n",
    "        fd, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "        hog_features.append(fd)\n",
    "    return hog_features\n",
    "\n",
    "# Compute HoG features for training and validation sets\n",
    "x_train_hog = compute_hog_features(x_train)\n",
    "x_valid_hog = compute_hog_features(x_valid)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x_train_hog = np.array(x_train_hog)\n",
    "x_valid_hog = np.array(x_valid_hog)\n",
    "\n",
    "print(f'Shape of HoG features - Training set: {x_train_hog.shape}')\n",
    "print(f'Shape of HoG features - Validation set: {x_valid_hog.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define batch size and other hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 35\n",
    "\n",
    "# Convert HoG features to PyTorch Tensors\n",
    "x_train_tensor = torch.tensor(x_train_hog, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_valid_tensor = torch.tensor(x_valid_hog, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create TensorDataset and DataLoader for validation\n",
    "valid_dataset = TensorDataset(x_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=144, out_features=120)  # 144 is the number of HoG features\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # Use HoG features directly\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)  # Use HoG features directly\n",
    "            loss = criterion(output, target)\n",
    "            total_valid_loss += loss.item()\n",
    "    \n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}')\n",
    "    \n",
    "    # Save the model with the best validation loss\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# Load the best model state dict\n",
    "if best_model_state_dict is not None:\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), 'best_model_p2.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded your test images into x_test (list/array of images)\n",
    "\n",
    "# Compute HoG features for test set\n",
    "x_test_hog = compute_hog_features(x_test)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x_test_hog = np.array(x_test_hog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Convert HoG features to numpy arrays (if not already done)\n",
    "x_test_hog = np.array(x_test_hog)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create TensorDataset and DataLoader for test set\n",
    "test_tensor = torch.tensor(x_test_hog, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "test_dataset = TensorDataset(test_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize your model (SimpleCNN) and load the best trained weights\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('best_model_p2.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test set and collect predictions and labels\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)  # Assuming HoG features are directly passed as input\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, range(10))\n",
    "plt.yticks(tick_marks, range(10))\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
